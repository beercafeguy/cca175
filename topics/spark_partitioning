$SPARK_HOME/bin/spark-shell \
--jars /user/beercafeguy/cca175/jar/commons-csv-1.5.jar \
--num-executors 5

val words= sc.parallelize("""we just sum all values for each partiton and sum the partition values 
	But with averages, itâ€™s not that simple, an average of averages is not the same as taking an 
	average across all numbers. But we can collect the total number scores and total score per partition 
	then divide the total overall score by the number of scores""".split(" "))


words.coalesce(1).partitions.length //collapsing partions in same machine

words.repartition(1).partitions.length
words.repartition(5).partitions.length //can increase or decrease number of partitions but requires shuffle


repartitionAndSortWithinPartitions(partitioner, num of partitions)


Custom partitioner

HashPartitioner //descrete values
RangePartitioner //continuous values

import org.apache.spark.HashPartitioner
import org.apache.spark.RangePartitioner

val nums=sc.range(1,1000)


class CustomerPArtitioner extends Partitioner{

	def numOfPArtitions=3
	def getPArtition(key:Any):Int={
		val customerId=key.asInstanceOf[Double].toInt
		if(customerId == 12345 || customerId == 12344){
			return 0
		}else {
			return new java.util.Random().nextInt(2)+1
		}
	}
}

rdd.partitionBy(new CustomerPArtitioner)


//new partition will be discussed in project

import org.apache.spark.HashPartitioner

val data=Seq((1,1),(2,4),(3,9),(4,16),(9,81))
val rdd=sc.parallelize(data).partitionBy(new HashPartitioner(4))

scala> rdd.partitioner
res17: Option[org.apache.spark.Partitioner] = Some(org.apache.spark.HashPartitioner@4)

scala> rdd.partitions.size
res18: Int = 4


rdd.foreachPartition(it =>{
println("Partition:")
it.foreach(t => t.toString)
println()	
})



val rdd = sc.parallelize(for {
x <- 1 to 3
y <- 1 to 2
} yield (x, None), 8)

rdd.take(10).foreach(println)

//function to get num of elements per partition.
import org.apache.spark.rdd.RDD
def getNumPerPartition(rdd:RDD[(Int,None.type)]){
rdd.mapPartitions( iter => Iterator(iter.length))
}


